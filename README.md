# E-commerce Data Pipeline - Daily Transactions Processing

This project implements an automated data pipeline for processing daily e-commerce transaction data, demonstrating best practices for data ingestion, transformation, and archiving using AWS services. 

## Project Overview

**Purpose:**

* To efficiently process and analyze daily e-commerce transactions, enabling data-driven insights and decision making.

**Workflow:**

1. **Data Generation:** A Lambda function generates mock e-commerce transaction data in CSV format, mimicking real-world scenarios. This data is then uploaded to an S3 bucket in a partitioned structure for efficient organization and retrieval.
2. **Data Ingestion & Processing:** A scheduled Glue ETL job runs daily to ingest the data from the S3 bucket. The job performs transformations and loads the cleaned data into a Redshift data warehouse.
3. **Monitoring & Notifications:** An EventBridge rule monitors the Glue job's status.  If there are any changes, an SNS topic sends notifications to subscribers, keeping stakeholders informed.
4. **Data Archiving:**  A final Lambda function, triggered by the SNS notification, archives the processed data to a separate S3 bucket for long-term storage and backup purposes.

## Architecture

* **Data Sources:**  Mock e-commerce transactions (generated by Lambda function)
* **Data Processing:** Lambda, S3, Glue ETL, Redshift
* **Monitoring & Alerts:** EventBridge, SNS
* **Data Archive:** S3 Bucket

## Key Features & Benefits

* **Automated Data Processing:**  The entire pipeline runs automatically, ensuring consistent and timely data updates.
* **Data Quality & Reliability:** Data quality checks are implemented within the Glue job to maintain data integrity.
* **Scalability & Flexibility:** The architecture can handle growing data volumes and supports integration with other AWS services.
* **Data Security & Compliance:**  The pipeline utilizes AWS security best practices to protect sensitive data.

## Getting Started

1. **Prerequisites:** AWS Account with appropriate permissions (IAM roles)
2. **Deployment:** 
    * Create the necessary AWS resources (S3 buckets, Redshift cluster, IAM roles).
    * Configure the Lambda functions, Glue ETL job, EventBridge rule, and SNS topic.
3. **Data Generation:** Trigger the data generation Lambda function to populate the S3 bucket with mock data.

## Project Details

* **Programming Languages:** Python
* **AWS Services:** Lambda, S3, Glue, Redshift, EventBridge, SNS
* **CI/CD:** AWS CodeBuild is used for continuous integration and deployment.
